---
layout: post
mathjax: true
---

### PCA가 뭘까?

**Principal Component Analysis (PCA)** 를 이용하여 데이터를 전처리하는 이유는 무엇일까? 데이터의 차원 축소 ? 분산이 최대가 되는 특성 찾기 ? PCA에는 여러 의미가 담겨져 있고, 뜬금없이 고유값 분해 (eigen value decomposition) 이 튀어나기도 한다. 이 포스팅에서는 PCA의 직관적인 이해로부터 출발해 완벽히 PCA를 이해하는것을 목표로 한다.

### PCA를 하는 이유

...

즉, PCA의 목표는 다음과 같은 $k$개의 새로운 feature $\textbf{v}_1,\textbf{v}_2,\cdots,\textbf{v}_k \in R^d$ 를 찾는 것이다.
$$ \textbf{x}_i \approx \sum_{j=1}^{k}a_{ij}\textbf{v}_j$$
이해하기 쉽게 k = 1 이라 가정해보면 다음과 같습니다. 

~그림

새로운 feature 혹은 새로운 축 $\textbf{v}$ 들을 이용하면, 각 $\textbf{v}$ 에 해당하는 좌표 $a$를 이용해 $k$ 개로 $\textbf{x}$ 를 **표현** 할 수 있습니다. 따라서 데이터 압축 등에 사용될 수 있습니다. 

### 계산하기

다시 한 번 강조하자면 PCA는 분산을 최대로 하는,기존 feature들 간 선형결합으로 만들어진 새로운 feature를 찾는 것입니다. 이 때 이 선형결합으로 탄생한 새로운 feature는 놀랍게도 고유값 분해로 부터 얻을 수 있습니다. 

데이터 $X$ 가 $n$ 개의 $\textbf{x}_1,\textbf{x}_2,\cdots \textbf{x}_n,$ 으로 이루어진 행렬이라 하고 각 데이터 $\textbf{x}$ 의 차원을 $d$ 라고 합시다.  $X$ 는 $(n,d)$ 행렬이 됩니다. 여기서 각 차원에 대한 평균을 빼서 centerize 시킨 새로운 행렬을 $Y$ 라 하면, $Y = X - \bar X$ 가 되고, $Y^TY$/n 는 X의 Covarianc 행렬이 됩니다. 여기서 $Y^TY$ 를 고유값 분해 하면, $Y^TY = U\sum U^T$ 가 되고, U의 column vector들이 우리가 찾던 $\textbf{v}$ 가 됩니다!

### 코딩하기

1. X를 Centrize 시킨다.

```
Y = X - X.mean(1)
```
2. SVD를 이용해 U를 구한다.
```
U,S,V = np.linalg.svd(np.dot(Y^T,Y))
```
3. U 에서 k개 만을 이용하여 X를 압축한다.
```
X_reduced = np.dot(X,U[:,:k])
```

곱씹어보면, 우리가 구한 X_reduced는 앞서 설명한 $a$ 들과 같습니다. 다른 의미로 우리는 새로운 축 $\textbf{v}$ 에서의 좌표 $a$ 를 구한 것입니다. 

### 응용하기
 앞서 말했듯 X_reduced 는 새로운 feature vector $\textbf{v}$ 에서의 새로운 좌표다. 즉 위에서 구한 `X_reduced` 는 새로운 기저벡터 $v$ 의 좌표들 $a$ 다. 이 
